{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Import librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "import os "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## RDD Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "sc = spark.sparkContext\r\n",
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### RDD - 1. Load CSV into RDDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Load data file \r\n",
        "\r\n",
        "contoso_dataset_path = \"abfss://dlsfssynwformwtwfrctrl@dlsaccsynwformwtwfrctrl.dfs.core.windows.net/synapse/workspaces/synw-formwtw-frctrl/warehouse/dataset/contoso/\"\r\n",
        "\r\n",
        "product_path = os.path.join(contoso_dataset_path, \"DimProduct.csv\")\r\n",
        "sales_path   = os.path.join(contoso_dataset_path, \"FactSales.csv\")\r\n",
        "\r\n",
        "rdd_products = sc.textFile(product_path)\r\n",
        "rdd_sales    = sc.textFile(sales_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "rdd_products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "rdd_sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Ignore headers and split:\r\n",
        "\r\n",
        "header_p = rdd_products.first()\r\n",
        "rdd_products = rdd_products.filter(lambda l: l != header_p).map(lambda l: l.split(\",\"))\r\n",
        "\r\n",
        "header_s = rdd_sales.first()\r\n",
        "rdd_sales = rdd_sales.filter(lambda l: l != header_s).map(lambda l: l.split(\",\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "header_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "header_s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### RDD - 2. Inspect RDDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(\"Product sample:\", rdd_products.take(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(\"Total products:\", rdd_products.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(\"Sales sample:\", rdd_sales.take(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(\"Total sales records:\", rdd_sales.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### RDD - 3. Perform Map, Filter, Reduce Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Extract product names\r\n",
        "product_names = rdd_products.map(lambda fields: fields[2])  # name at index 2\r\n",
        "print(\"Sample names:\", product_names.take(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Filter expensive products (> $1000)\r\n",
        "expensive = rdd_products.filter(lambda fields: float(fields[4]) > 1000.0)  # price at idx 4\r\n",
        "# print(\"Expensive products:\", expensive.take(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Map-product to sales join via keyBy\r\n",
        "# product_id assumed at index 0; sale fields: product_id idx 2, quantity at idx 5\r\n",
        "prod_kv = rdd_products.map(lambda f: (f[0], f))\r\n",
        "sales_kv = rdd_sales.map(lambda f: (f[2], f))\r\n",
        "\r\n",
        "# join and create records (prod, qty)\r\n",
        "prod_sales = prod_kv.join(sales_kv).map(lambda kv: (kv[1][0][2], int(kv[1][1][5])))\r\n",
        "print(\"Product‚ÄìSales sample:\", prod_sales.take(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Aggregate total quantity sold per product\r\n",
        "total_per_prod = prod_sales.reduceByKey(lambda x, y: x + y)\r\n",
        "print(\"Totals:\", total_per_prod.take(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### RDD - 4. Advanced Transformation: Word Count from Product Descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {},
      "source": [
        "# assuming description at idx 3\r\n",
        "words = rdd_products.flatMap(lambda f: f[3].split(\" \"))\r\n",
        "word_counts = words.map(lambda w: (w.lower(), 1)).reduceByKey(lambda a, b: a + b)\r\n",
        "print(\"Top words:\", word_counts.takeOrdered(10, key=lambda kv: -kv[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### RDD - 5. Convert RDD to DataFrame & Use SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import Row\r\n",
        "\r\n",
        "# Convert product totals\r\n",
        "row_rdd = total_per_prod.map(lambda kv: Row(product_name=kv[0], qty_sold=kv[1]))\r\n",
        "df_totals = spark.createDataFrame(row_rdd)\r\n",
        "\r\n",
        "df_totals.createOrReplaceTempView(\"prod_totals\")\r\n",
        "spark.sql(\"SELECT * FROM prod_totals ORDER BY qty_sold DESC\").show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### ‚úÖ Summary\r\n",
        "    Loading raw CSVs into RDDs\r\n",
        "    Transforming data with map, filter, reduceByKey, and flatMap\r\n",
        "    Joining product and sales data via key-pairs\r\n",
        "    Aggregating insights using RDD logic\r\n",
        "    Converting to DataFrames for SQL or visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## FEATURES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### üßæ Feature Matrix\r\n",
        "\r\n",
        "\r\n",
        "| Feature                            | üüß Databricks Serverless | üü¶ Databricks Standard Cluster | üî∑ Synapse Spark Pool | ‚ö´ Apache Spark (Standard Cluster) |\r\n",
        "| ---------------------------------- | ------------------------ | ------------------------------ | --------------------- | --------------------------------- |\r\n",
        "| **`sparkContext` access**          | ‚ùå Blocked                | ‚úÖ Full Access                  | ‚úÖ Full Access         | ‚úÖ Full Access                     |\r\n",
        "| **RDD creation (`parallelize`)**   | ‚ùå Not supported          | ‚úÖ Yes                          | ‚úÖ Yes                 | ‚úÖ Yes                             |\r\n",
        "| **JVM access (`_jvm`, broadcast)** | ‚ùå Not allowed            | ‚úÖ Yes                          | ‚úÖ Yes                 | ‚úÖ Yes                             |\r\n",
        "| **`df.cache()`** (memory caching)  | ‚ùå Ignored or fails       | ‚úÖ Supported                    | ‚úÖ Supported           | ‚úÖ Supported                       |\r\n",
        "| **`DataFrame` API**                | ‚úÖ Recommended            | ‚úÖ Recommended                  | ‚úÖ Recommended         | ‚úÖ Recommended                     |\r\n",
        "| **Best use case**                  | Interactive SQL & BI     | Full Spark / ML workloads      | Data engineering + ML | Full control (custom Spark apps)  |\r\n",
        "| **Resource control**               | Abstracted               | Full control                   | Semi-managed          | Full control                      |\r\n",
        "| **Supports custom Spark configs**  | ‚ùå Very limited           | ‚úÖ Fully customizable           | ‚ö†Ô∏è Limited            | ‚úÖ Fully customizable              |\r\n",
        "| **Cluster reuse across users**     | ‚úÖ Auto scale & share     | ‚úÖ Manual control               | ‚úÖ Session-pool based  | ‚úÖ Depends on setup                |\r\n",
        "| **Interactive performance tuning** | ‚ùå Limited                | ‚úÖ Yes                          | ‚ö†Ô∏è Limited            | ‚úÖ Full control                    |\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "description": "RDD Operations - WTW PySpark Session",
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}