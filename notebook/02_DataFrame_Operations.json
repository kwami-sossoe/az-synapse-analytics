{
	"name": "02_DataFrame_Operations",
	"properties": {
		"description": "DataFrame Operations - WTW PySpark Session",
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "synspformwtw001",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "53638473-5550-4313-91f0-2bfe20737ea2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/2716549b-013e-4f5c-bfaf-76bfdd78083a/resourceGroups/RG_FORM_WTW/providers/Microsoft.Synapse/workspaces/synw-formwtw-frctrl/bigDataPools/synspformwtw001",
				"name": "synspformwtw001",
				"type": "Spark",
				"endpoint": "https://synw-formwtw-frctrl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/synspformwtw001",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"run_control": {
						"frozen": true
					},
					"editable": false
				},
				"source": [
					"spark"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import librairies"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"run_control": {
						"frozen": false
					},
					"editable": true
				},
				"source": [
					"import os\r\n",
					"from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType"
				],
				"execution_count": 114
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## DATAFRAME OPERATIONS"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Functions Utility"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def load_table(table_name: str, db_name=None):\r\n",
					"    \"\"\"Load a table from the specified database.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        table_name (str): The name of the table to load.\r\n",
					"        db_name (str, optional): The name of the database. Defaults to None.\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        DataFrame: The loaded table as a DataFrame.\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    if db_name:\r\n",
					"        table = db_name + \".\" + table_name\r\n",
					"    else:\r\n",
					"        table = \"default\" + \".\" + table_name\r\n",
					"\r\n",
					"    try:\r\n",
					"        return spark.read.table(table)\r\n",
					"    except Exception as e:\r\n",
					"        print(f\"Error loading data: {e}\")\r\n",
					"        print(f\"Please ensure the {table_name} table exist.\")"
				],
				"execution_count": 115
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### DataFrame - Cas Pratique 1 - Schéma de quelques tables"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"contoso_dataset_path = \"abfss://dlsfssynwformwtwfrctrl@dlsaccsynwformwtwfrctrl.dfs.core.windows.net/synapse/workspaces/synw-formwtw-frctrl/warehouse/dataset/contoso/\"\r\n",
					"\r\n",
					"dim_customer_path = os.path.join(contoso_dataset_path, \"DimCustomer.csv\")\r\n",
					"dim_product_path   = os.path.join(contoso_dataset_path, \"DimProduct.csv\")\r\n",
					"dim_store_path   = os.path.join(contoso_dataset_path, \"DimStore.csv\")\r\n",
					"dim_factSales_path   = os.path.join(contoso_dataset_path, \"FactOnlineSales.csv\")"
				],
				"execution_count": 116
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"try:\r\n",
					"    df_dim_customer = spark.read.csv(dim_customer_path, header=True, inferSchema=True)\r\n",
					"    df_dim_product = spark.read.csv(dim_product_path, header=True, inferSchema=True)\r\n",
					"    df_dim_store = spark.read.csv(dim_store_path, header=True, inferSchema=True)\r\n",
					"    df_factSales = spark.read.csv(dim_factSales_path, header=True, inferSchema=True)\r\n",
					"except Exception as e:\r\n",
					"    print(f\"Error loading data: {e}\")\r\n",
					"    print(\"Please ensure the data_path is correct and the CSV files exist.\")\r\n",
					"    # spark.stop()"
				],
				"execution_count": 117
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"df_dim_customer.printSchema()\r\n",
					"display(df_dim_customer.limit(5))"
				],
				"execution_count": 118
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"df_dim_product.printSchema()\r\n",
					"display(df_dim_product.limit(5))"
				],
				"execution_count": 119
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"df_dim_store.printSchema()\r\n",
					"display(df_dim_store.limit(5))"
				],
				"execution_count": 120
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"df_factSales.printSchema()\r\n",
					"display(df_factSales.limit(5))"
				],
				"execution_count": 121
			},
			{
				"cell_type": "code",
				"source": [
					"# Create Tables\r\n",
					"\r\n",
					"try:\r\n",
					"    \r\n",
					"    df_dim_customer.createOrReplaceTempView(\"table_customer\")\r\n",
					"\r\n",
					"    df_dim_product.createOrReplaceTempView(\"table_product\")\r\n",
					"\r\n",
					"    df_dim_store.createOrReplaceTempView(\"table_store\")\r\n",
					"\r\n",
					"    df_factSales.createOrReplaceTempView(\"table_fact_sales\")\r\n",
					"\r\n",
					"except Exception as e:\r\n",
					"    print(f\"Error creating table: {e}\")\r\n",
					"    print(\"Please ensure the dataframe is already created and available in memory.\")"
				],
				"execution_count": 122
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# REQUETES SPARK SQL\r\n",
					"\r\n",
					"display(spark.sql(\"SELECT * FROM table_customer\").limit(3))\r\n",
					"\r\n",
					"display(spark.sql(\"SELECT FirstName, LastName, NumberChildrenAtHome, Education FROM table_customer\").limit(5))"
				],
				"execution_count": 123
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Jointures"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": true
					},
					"collapsed": false
				},
				"source": [
					"# Jointure df_factSales avec df_dim_customer (INNER JOIN)\r\n",
					"sales_with_customers = df_factSales.join(df_dim_customer, on=\"CustomerKey\", how=\"inner\")\r\n",
					"display(sales_with_customers.select(\"OnlineSalesKey\", \"FirstName\", \"LastName\", \"ProductKey\", \"TotalCost\").limit(20))"
				],
				"execution_count": 124
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": true
					},
					"collapsed": false
				},
				"source": [
					"# Variante : LEFT JOIN (pour garder toutes les ventes même sans client connu)\r\n",
					"sales_with_customers = df_factSales.join(df_dim_customer, on=\"CustomerKey\", how=\"left\")\r\n",
					"display(sales_with_customers.limit(10))"
				],
				"execution_count": 125
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": true
					}
				},
				"source": [
					"# Jointure df_factSales avec product_df (INNER JOIN)\r\n",
					"\r\n",
					"# Pour analyser les ventes par produit, catégorie, prix\r\n",
					"\r\n",
					"sales_with_products = df_factSales.join(df_dim_product, on=\"ProductKey\", how=\"inner\")\r\n",
					"sales_with_products.columns\r\n",
					"\r\n",
					"# ########### sales_with_products.select(\"OnlineSalesKey\", \"ProductName\", \"ProductKey\", \"UnitPrice\", \"TotalCost\").show()"
				],
				"execution_count": 126
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": true
					}
				},
				"source": [
					"# Jointure df_factSales avec df_dim_store (INNER JOIN)\r\n",
					"\r\n",
					"sales_with_store = df_factSales.join(df_dim_store, on=\"StoreKey\", how=\"inner\")\r\n",
					"\r\n",
					"sales_with_store.select(\"OnlineSalesKey\", \"StoreName\", \"GeographyKey\", \"TotalCost\").show()"
				],
				"execution_count": 127
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": true
					}
				},
				"source": [
					"# Chaînage : df_factSales + df_dim_product + df_dim_customer + df_dim_store\r\n",
					"full_sales = df_factSales \\\r\n",
					"    .join(df_dim_product, on=\"ProductKey\", how=\"inner\") \\\r\n",
					"    .join(df_dim_customer, on=\"CustomerKey\", how=\"inner\") \\\r\n",
					"    .join(df_dim_store, on=\"StoreKey\", how=\"inner\")\r\n",
					"\r\n",
					"full_sales.select(\"OnlineSalesKey\", \"FirstName\", \"ProductKey\", \"StoreName\", \"TotalCost\").show()\r\n",
					""
				],
				"execution_count": 128
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Cas Pratique 2 - Ingestion + Lecture des données dans l’ADLS"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"try:\r\n",
					"    df_dim_customer.cache()\r\n",
					"    df_dim_product.cache()\r\n",
					"    df_dim_store.cache()\r\n",
					"    df_factSales.cache()\r\n",
					"except Exception as e:\r\n",
					"    print(f\"Error caching dataframe: {e}\")\r\n",
					"    print(\"Please ensure the dataframe exist in memory.\")\r\n",
					"    # spark.stop()"
				],
				"execution_count": 129
			},
			{
				"cell_type": "code",
				"source": [
					"# Créer differentes tables\r\n",
					"try:\r\n",
					"    \r\n",
					"    df_dim_customer.createOrReplaceTempView(\"table_customer\")\r\n",
					"\r\n",
					"    df_dim_product.createOrReplaceTempView(\"table_product\")\r\n",
					"\r\n",
					"    df_dim_store.createOrReplaceTempView(\"table_store\")\r\n",
					"\r\n",
					"    df_factSales.createOrReplaceTempView(\"table_fact_sales\")\r\n",
					"\r\n",
					"except Exception as e:\r\n",
					"    print(f\"Error creating table: {e}\")\r\n",
					"    print(\"Please ensure the dataframe is already created and available in memory.\")\r\n",
					"    # spark.stop()"
				],
				"execution_count": 130
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Cas Pratique 3 - Manipulations simples de dataframe"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import os "
				],
				"execution_count": 131
			},
			{
				"cell_type": "code",
				"source": [
					"contoso_dataset_path = \"abfss://synapsestorage-fs-skw@synapsestorageskw.dfs.core.windows.net/contoso_dataset/\"\r\n",
					"\r\n",
					"filename = \"DimCustomer.csv\"\r\n",
					"\r\n",
					"file_path = os.path.join(contoso_dataset_path, filename)\r\n",
					"\r\n",
					"print(f\"file_path {file_path}\")"
				],
				"execution_count": 132
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					},
					"collapsed": false
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"df_dim_customer = spark.read.csv(dim_customer_path, header=True, inferSchema=True)\r\n",
					"display(df_dim_customer.limit(10))"
				],
				"execution_count": 133
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Cas Pratique 4 - Utilisation de Spark SQL"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# 1 – Créer une vue temporaire\r\n",
					"\r\n",
					"# Jointure pour enrichir les ventes avec les noms de produits\r\n",
					"sales_with_products = df_factSales.join(df_dim_product, on=\"ProductKey\", how=\"inner\")\r\n",
					"\r\n",
					"# Création de la vue temporaire\r\n",
					"sales_with_products.createOrReplaceTempView(\"sales_view\")"
				],
				"execution_count": 144
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Faire du caching, au besoin\r\n",
					"- df_dim_customer.cache()\r\n",
					"- df_dim_product.cache()\r\n",
					"- df_dim_store.cache()\r\n",
					"- df_factSales.cache()"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"sales_with_products.columns"
				],
				"execution_count": 135
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Créer une vue temporaire"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"SELECT \r\n",
					"    ProductName, \r\n",
					"    SUM(TotalCost) AS total_revenue\r\n",
					"FROM \r\n",
					"    sales_view\r\n",
					"GROUP BY \r\n",
					"    ProductName"
				],
				"execution_count": 142
			},
			{
				"cell_type": "code",
				"source": [
					"# Exécution de la requête SQL et stockage du résultat\r\n",
					"df_top_products = spark.sql(\"\"\"\r\n",
					"    SELECT \r\n",
					"        ProductName, \r\n",
					"        SUM(TotalCost) AS total_revenue\r\n",
					"    FROM \r\n",
					"        sales_view\r\n",
					"    GROUP BY \r\n",
					"        ProductName\r\n",
					"\"\"\")"
				],
				"execution_count": 143
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# Trier et afficher les 10 produits les plus vendus\r\n",
					"df_top_10_products = df_top_products.orderBy(\"total_revenue\", ascending=False).limit(10)\r\n",
					"display(df_top_10_products.limit(10))"
				],
				"execution_count": 138
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Cas Pratique 5 - Spécification de schéma"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Définir un schéma explicite pour la table Customer\r\n",
					"\r\n",
					"from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\r\n",
					"\r\n",
					"customer_schema = StructType([\r\n",
					"    StructField(\"CustomerKey\", IntegerType(), True),\r\n",
					"    StructField(\"FirstName\", StringType(), True),\r\n",
					"    StructField(\"LastName\", StringType(), True),\r\n",
					"    StructField(\"YearlyIncome\", DoubleType(), True),\r\n",
					"    StructField(\"TotalChildren\", IntegerType(), True)\r\n",
					"])"
				],
				"execution_count": 139
			},
			{
				"cell_type": "code",
				"source": [
					"# Lire un fichier CSV en appliquant un schéma custom\r\n",
					"\r\n",
					"df_dim_customer_custom = spark.read.csv(\r\n",
					"    path=dim_customer_path,\r\n",
					"    schema=customer_schema,\r\n",
					"    header=True,\r\n",
					"    sep=\",\",\r\n",
					"    inferSchema=False\r\n",
					")"
				],
				"execution_count": 141
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": true
					}
				},
				"source": [
					"# Afficher le schéma et les 5 premières lignes\r\n",
					"\r\n",
					"df_dim_customer_custom.printSchema()\r\n",
					"df_dim_customer_custom.show(5)"
				],
				"execution_count": 145
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Cas Pratique 6 - Utilisation des jointures"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# 1. Lister les clients avec leurs achats\r\n",
					"\r\n",
					"# Jointure entre clients et ventes\r\n",
					"clients_avec_achats = df_dim_customer.join(\r\n",
					"    df_factSales, \r\n",
					"    on=\"CustomerKey\", \r\n",
					"    how=\"inner\"\r\n",
					")\r\n",
					"\r\n",
					"# Sélection des colonnes demandées\r\n",
					"clients_avec_achats.select(\r\n",
					"    \"FirstName\", \"LastName\", \"SalesOrderNumber\", \"SalesAmount\"\r\n",
					").show(10)"
				],
				"execution_count": 146
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": true
					}
				},
				"source": [
					"# 2. Identifier les clients sans aucun achat\r\n",
					"\r\n",
					"clients_sans_achat = df_dim_customer.join(\r\n",
					"    df_factSales, \r\n",
					"    on=\"CustomerKey\", \r\n",
					"    how=\"left_anti\"\r\n",
					")\r\n",
					"\r\n",
					"clients_sans_achat.select(\"FirstName\", \"LastName\").show(10)"
				],
				"execution_count": 147
			},
			{
				"cell_type": "code",
				"source": [
					"# 3. Ajouter le nom du produit acheté (jointure avec Product)\r\n",
					"\r\n",
					"# Jointure à trois tables : Customer + OnlineSales + Product\r\n",
					"clients_achats_produits = df_dim_customer \\\r\n",
					"    .join(df_factSales, on=\"CustomerKey\", how=\"inner\") \\\r\n",
					"    .join(df_dim_product, on=\"ProductKey\", how=\"inner\")\r\n",
					"\r\n",
					"# Sélection enrichie avec le nom du produit\r\n",
					"clients_achats_produits.select(\r\n",
					"    \"FirstName\", \"LastName\", \"SalesOrderNumber\", \"SalesAmount\", \"ProductName\"\r\n",
					").show(10)"
				],
				"execution_count": 148
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Cas Pratique 7 - Cas pratique complet"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Sauvegarde de résultats en mode parquet"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": true
					}
				},
				"source": [
					"clients_achats_produits.columns"
				],
				"execution_count": 149
			},
			{
				"cell_type": "code",
				"source": [
					"new_df = clients_achats_produits[['ProductKey', 'CustomerKey', '_c0', 'GeographyKey', 'FirstName', 'LastName', 'BirthDate', 'MaritalStatus',\r\n",
					" 'Gender', 'YearlyIncome', 'TotalChildren', 'NumberChildrenAtHome', 'Education']]\r\n",
					"\r\n",
					"new_df.printSchema()\r\n",
					"\r\n",
					"new_df.show(5)"
				],
				"execution_count": 150
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### mode d'enregistrement des dataframes en format parquet : \r\n",
					"    - overwrite,\r\n",
					"    - append,\r\n",
					"    - ignore,\r\n",
					"    - error\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"output_dataset_path = \"abfss://dlsfssynwformwtwfrctrl@dlsaccsynwformwtwfrctrl.dfs.core.windows.net/synapse/workspaces/synw-formwtw-frctrl/warehouse/dataset/out_contoso/\"\r\n",
					"\r\n",
					"new_df.write.mode(\"overwrite\").parquet(output_dataset_path + \"CA_par_produits\")"
				],
				"execution_count": 151
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Cas Pratique 8 - Cas pratique complet"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### 1. Chargement des fichiers CSV"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Chargement des fichiers CSV\r\n",
					"\r\n",
					"try:\r\n",
					"    dim_customer_df = spark.read.csv(dim_customer_path, header=True, inferSchema=True)\r\n",
					"    dim_product_df = spark.read.csv(dim_product_path, header=True, inferSchema=True)\r\n",
					"    dim_store_df = spark.read.csv(dim_store_path, header=True, inferSchema=True)\r\n",
					"    fact_sales_df = spark.read.csv(dim_factSales_path, header=True, inferSchema=True)\r\n",
					"except Exception as e:\r\n",
					"    print(f\"Error loading data: {e}\")\r\n",
					"    print(\"Please ensure the data_path is correct and the CSV files exist.\")"
				],
				"execution_count": 152
			},
			{
				"cell_type": "code",
				"source": [
					"# Chargement des fichiers CSV (suite)\r\n",
					"\r\n",
					"dim_date_path   = os.path.join(contoso_dataset_path, \"DimDate.csv\")\r\n",
					"dim_geo_path   = os.path.join(contoso_dataset_path, \"DimGeography.csv\")\r\n",
					"\r\n",
					"try:\r\n",
					"    dim_date_df = spark.read.csv(dim_date_path, header=True, inferSchema=True)\r\n",
					"    dim_geo_df = spark.read.csv(dim_geo_path, header=True, inferSchema=True)\r\n",
					"except Exception as e:\r\n",
					"    print(f\"Error loading data: {e}\")\r\n",
					"    print(\"Please ensure the data_path is correct and the CSV files exist.\")"
				],
				"execution_count": 158
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### 2. Nettoyage et Preparation"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"fact_sales_df.columns"
				],
				"execution_count": 154
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# Supprimer les lignes avec des valeurs nulles critiques\r\n",
					"\r\n",
					"fact_sales_df = fact_sales_df.dropna(subset=[\"ProductKey\", \"DateKey\", \"SalesAmount\"])\r\n",
					"\r\n",
					"# Convertir les dates (dans DimDate) et montants (SalesAmount)\r\n",
					"\r\n",
					"from pyspark.sql.functions import to_date, col, concat_ws\r\n",
					"\r\n",
					"# Conversion de la date\r\n",
					"\"\"\"\r\n",
					"dim_date_df = dim_date_df.withColumn(\r\n",
					"    \"FullDate\", to_date(concat_ws(\"-\", col(\"CalendarYear\"), col(\"MonthNumber\"), col(\"DateKey\")), \"yyyy-M-d\")\r\n",
					")\r\n",
					"\"\"\"\r\n",
					"\r\n",
					"dim_date_df = dim_date_df.withColumn(\r\n",
					"    \"FullDate\", to_date(concat_ws(\"-\", col(\"DateKey\")), \"yyyy-M-d\")\r\n",
					")\r\n",
					"\r\n",
					"# Ajout colonne YearMonth (ex : 2024-06)\r\n",
					"dim_date_df = dim_date_df.withColumn(\r\n",
					"    \"YearMonth\", concat_ws(\"-\", col(\"Year\"), col(\"Month\"))\r\n",
					")\r\n",
					"\r\n",
					"# Conversion des montants en float si nécessaire\r\n",
					"fact_sales_df = fact_sales_df.withColumn(\"SalesAmount\", col(\"SalesAmount\").cast(\"double\"))\r\n",
					"\r\n",
					"display(dim_date_df.limit(10))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### 3. Jointure — Création de la vue enrichie des ventes"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": true
					}
				},
				"source": [
					"\r\n",
					"sales_enriched_df = fact_sales_df \\\r\n",
					"    .join(dim_date_df.select(\"DateKey\", \"FullDate\", \"YearMonth\"), on=\"DateKey\", how=\"inner\") \\\r\n",
					"    .join(dim_customer_df.select(\"CustomerKey\", \"GeographyKey\"), on=\"CustomerKey\", how=\"left\") \\\r\n",
					"    .join(dim_geo_df.select(\"GeographyKey\", \"RegionCountryName\"), on=\"GeographyKey\", how=\"left\") \\\r\n",
					"    .join(dim_product_df.select(\"ProductKey\", \"ProductName\", \"ProductCategoryName\"), on=\"ProductKey\", how=\"left\") \\\r\n",
					"    .join(dim_store_df.select(\"StoreKey\", \"StoreName\", \"StoreType\"), on=\"StoreKey\", how=\"left\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### 4.  KPI à Calculer"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# KPI 1 – Chiffre d'affaires mensuel par produit et région\r\n",
					"\r\n",
					"monthly_revenue = sales_enriched_df.groupBy(\"YearMonth\", \"ProductName\", \"RegionCountryName\") \\\r\n",
					"    .agg({\"SalesAmount\": \"sum\"}) \\\r\n",
					"    .withColumnRenamed(\"sum(SalesAmount)\", \"MonthlyRevenue\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# KPI 2 – Top 10 produits les plus vendus (volume et valeur)\r\n",
					"\r\n",
					"from pyspark.sql.functions import sum as _sum\r\n",
					"\r\n",
					"top_products = sales_enriched_df.groupBy(\"ProductName\") \\\r\n",
					"    .agg(\r\n",
					"        _sum(\"SalesAmount\").alias(\"TotalSales\"),\r\n",
					"        _sum(\"SalesQuantity\").alias(\"TotalUnits\")\r\n",
					"    ) \\\r\n",
					"    .orderBy(col(\"TotalSales\").desc())\r\n",
					"\r\n",
					"top_products.show(10)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# KPI 3 – Taux de croissance mensuel des ventes\r\n",
					"\r\n",
					"from pyspark.sql.window import Window\r\n",
					"from pyspark.sql.functions import lag, round\r\n",
					"\r\n",
					"monthly_sales = sales_enriched_df.groupBy(\"YearMonth\").agg(_sum(\"SalesAmount\").alias(\"TotalSales\"))\r\n",
					"\r\n",
					"w = Window.orderBy(\"YearMonth\")\r\n",
					"\r\n",
					"monthly_growth = monthly_sales.withColumn(\r\n",
					"    \"PreviousMonthSales\", lag(\"TotalSales\").over(w)\r\n",
					").withColumn(\r\n",
					"    \"GrowthRate\", round((col(\"TotalSales\") - col(\"PreviousMonthSales\")) / col(\"PreviousMonthSales\") * 100, 2)\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# KPI 4 – Répartition des ventes par canal (magasin physique vs en ligne)\r\n",
					"\r\n",
					"channel_split = sales_enriched_df.groupBy(\"StoreType\") \\\r\n",
					"    .agg(_sum(\"SalesAmount\").alias(\"TotalSales\")) \\\r\n",
					"    .orderBy(\"TotalSales\", ascending=False)\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# KPI 5 – Analyse RFM (Récence, Fréquence, Montant)\r\n",
					"\r\n",
					"from pyspark.sql.functions import max, datediff, countDistinct\r\n",
					"\r\n",
					"# Dernière date d'achat\r\n",
					"latest_date = sales_enriched_df.agg({\"FullDate\": \"max\"}).collect()[0][0]\r\n",
					"\r\n",
					"rfm = sales_enriched_df.groupBy(\"CustomerKey\") \\\r\n",
					"    .agg(\r\n",
					"        datediff(lit(latest_date), max(\"FullDate\")).alias(\"Recency\"),\r\n",
					"        countDistinct(\"SalesOrderNumber\").alias(\"Frequency\"),\r\n",
					"        _sum(\"SalesAmount\").alias(\"Monetary\")\r\n",
					"    )\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import IntegerType, UserDefinedType\r\n",
					"import random\r\n",
					"random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Have fun with PySpark !"
				]
			}
		]
	}
}