{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "run_control": {
          "frozen": true
        },
        "editable": false
      },
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Import librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "outputs": [],
      "metadata": {
        "run_control": {
          "frozen": false
        },
        "editable": true
      },
      "source": [
        "import os\r\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## DATAFRAME OPERATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Functions Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "outputs": [],
      "metadata": {},
      "source": [
        "def load_table(table_name: str, db_name=None):\r\n",
        "    \"\"\"Load a table from the specified database.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        table_name (str): The name of the table to load.\r\n",
        "        db_name (str, optional): The name of the database. Defaults to None.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        DataFrame: The loaded table as a DataFrame.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    if db_name:\r\n",
        "        table = db_name + \".\" + table_name\r\n",
        "    else:\r\n",
        "        table = \"default\" + \".\" + table_name\r\n",
        "\r\n",
        "    try:\r\n",
        "        return spark.read.table(table)\r\n",
        "    except Exception as e:\r\n",
        "        print(f\"Error loading data: {e}\")\r\n",
        "        print(f\"Please ensure the {table_name} table exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### DataFrame - Cas Pratique 1 - Schéma de quelques tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "outputs": [],
      "metadata": {},
      "source": [
        "contoso_dataset_path = \"abfss://dlsfssynwformwtwfrctrl@dlsaccsynwformwtwfrctrl.dfs.core.windows.net/synapse/workspaces/synw-formwtw-frctrl/warehouse/dataset/contoso/\"\r\n",
        "\r\n",
        "dim_customer_path = os.path.join(contoso_dataset_path, \"DimCustomer.csv\")\r\n",
        "dim_product_path   = os.path.join(contoso_dataset_path, \"DimProduct.csv\")\r\n",
        "dim_store_path   = os.path.join(contoso_dataset_path, \"DimStore.csv\")\r\n",
        "dim_factSales_path   = os.path.join(contoso_dataset_path, \"FactOnlineSales.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "try:\r\n",
        "    df_dim_customer = spark.read.csv(dim_customer_path, header=True, inferSchema=True)\r\n",
        "    df_dim_product = spark.read.csv(dim_product_path, header=True, inferSchema=True)\r\n",
        "    df_dim_store = spark.read.csv(dim_store_path, header=True, inferSchema=True)\r\n",
        "    df_factSales = spark.read.csv(dim_factSales_path, header=True, inferSchema=True)\r\n",
        "except Exception as e:\r\n",
        "    print(f\"Error loading data: {e}\")\r\n",
        "    print(\"Please ensure the data_path is correct and the CSV files exist.\")\r\n",
        "    # spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df_dim_customer.printSchema()\r\n",
        "display(df_dim_customer.limit(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df_dim_product.printSchema()\r\n",
        "display(df_dim_product.limit(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df_dim_store.printSchema()\r\n",
        "display(df_dim_store.limit(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df_factSales.printSchema()\r\n",
        "display(df_factSales.limit(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create Tables\r\n",
        "\r\n",
        "try:\r\n",
        "    \r\n",
        "    df_dim_customer.createOrReplaceTempView(\"table_customer\")\r\n",
        "\r\n",
        "    df_dim_product.createOrReplaceTempView(\"table_product\")\r\n",
        "\r\n",
        "    df_dim_store.createOrReplaceTempView(\"table_store\")\r\n",
        "\r\n",
        "    df_factSales.createOrReplaceTempView(\"table_fact_sales\")\r\n",
        "\r\n",
        "except Exception as e:\r\n",
        "    print(f\"Error creating table: {e}\")\r\n",
        "    print(\"Please ensure the dataframe is already created and available in memory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# REQUETES SPARK SQL\r\n",
        "\r\n",
        "display(spark.sql(\"SELECT * FROM table_customer\").limit(3))\r\n",
        "\r\n",
        "display(spark.sql(\"SELECT FirstName, LastName, NumberChildrenAtHome, Education FROM table_customer\").limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Jointures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "collapsed": false
      },
      "source": [
        "# Jointure df_factSales avec df_dim_customer (INNER JOIN)\r\n",
        "sales_with_customers = df_factSales.join(df_dim_customer, on=\"CustomerKey\", how=\"inner\")\r\n",
        "display(sales_with_customers.select(\"OnlineSalesKey\", \"FirstName\", \"LastName\", \"ProductKey\", \"TotalCost\").limit(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "collapsed": false
      },
      "source": [
        "# Variante : LEFT JOIN (pour garder toutes les ventes même sans client connu)\r\n",
        "sales_with_customers = df_factSales.join(df_dim_customer, on=\"CustomerKey\", how=\"left\")\r\n",
        "display(sales_with_customers.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# Jointure df_factSales avec product_df (INNER JOIN)\r\n",
        "\r\n",
        "# Pour analyser les ventes par produit, catégorie, prix\r\n",
        "\r\n",
        "sales_with_products = df_factSales.join(df_dim_product, on=\"ProductKey\", how=\"inner\")\r\n",
        "sales_with_products.columns\r\n",
        "\r\n",
        "# ########### sales_with_products.select(\"OnlineSalesKey\", \"ProductName\", \"ProductKey\", \"UnitPrice\", \"TotalCost\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# Jointure df_factSales avec df_dim_store (INNER JOIN)\r\n",
        "\r\n",
        "sales_with_store = df_factSales.join(df_dim_store, on=\"StoreKey\", how=\"inner\")\r\n",
        "\r\n",
        "sales_with_store.select(\"OnlineSalesKey\", \"StoreName\", \"GeographyKey\", \"TotalCost\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# Chaînage : df_factSales + df_dim_product + df_dim_customer + df_dim_store\r\n",
        "full_sales = df_factSales \\\r\n",
        "    .join(df_dim_product, on=\"ProductKey\", how=\"inner\") \\\r\n",
        "    .join(df_dim_customer, on=\"CustomerKey\", how=\"inner\") \\\r\n",
        "    .join(df_dim_store, on=\"StoreKey\", how=\"inner\")\r\n",
        "\r\n",
        "full_sales.select(\"OnlineSalesKey\", \"FirstName\", \"ProductKey\", \"StoreName\", \"TotalCost\").show()\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Cas Pratique 2 - Ingestion + Lecture des données dans l’ADLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "try:\r\n",
        "    df_dim_customer.cache()\r\n",
        "    df_dim_product.cache()\r\n",
        "    df_dim_store.cache()\r\n",
        "    df_factSales.cache()\r\n",
        "except Exception as e:\r\n",
        "    print(f\"Error caching dataframe: {e}\")\r\n",
        "    print(\"Please ensure the dataframe exist in memory.\")\r\n",
        "    # spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Créer differentes tables\r\n",
        "try:\r\n",
        "    \r\n",
        "    df_dim_customer.createOrReplaceTempView(\"table_customer\")\r\n",
        "\r\n",
        "    df_dim_product.createOrReplaceTempView(\"table_product\")\r\n",
        "\r\n",
        "    df_dim_store.createOrReplaceTempView(\"table_store\")\r\n",
        "\r\n",
        "    df_factSales.createOrReplaceTempView(\"table_fact_sales\")\r\n",
        "\r\n",
        "except Exception as e:\r\n",
        "    print(f\"Error creating table: {e}\")\r\n",
        "    print(\"Please ensure the dataframe is already created and available in memory.\")\r\n",
        "    # spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Cas Pratique 3 - Manipulations simples de dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "outputs": [],
      "metadata": {},
      "source": [
        "import os "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "outputs": [],
      "metadata": {},
      "source": [
        "contoso_dataset_path = \"abfss://synapsestorage-fs-skw@synapsestorageskw.dfs.core.windows.net/contoso_dataset/\"\r\n",
        "\r\n",
        "filename = \"DimCustomer.csv\"\r\n",
        "\r\n",
        "file_path = os.path.join(contoso_dataset_path, filename)\r\n",
        "\r\n",
        "print(f\"file_path {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "df_dim_customer = spark.read.csv(dim_customer_path, header=True, inferSchema=True)\r\n",
        "display(df_dim_customer.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Cas Pratique 4 - Utilisation de Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "outputs": [],
      "metadata": {},
      "source": [
        "# 1 – Créer une vue temporaire\r\n",
        "\r\n",
        "# Jointure pour enrichir les ventes avec les noms de produits\r\n",
        "sales_with_products = df_factSales.join(df_dim_product, on=\"ProductKey\", how=\"inner\")\r\n",
        "\r\n",
        "# Création de la vue temporaire\r\n",
        "sales_with_products.createOrReplaceTempView(\"sales_view\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Faire du caching, au besoin\r\n",
        "- df_dim_customer.cache()\r\n",
        "- df_dim_product.cache()\r\n",
        "- df_dim_store.cache()\r\n",
        "- df_factSales.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "outputs": [],
      "metadata": {},
      "source": [
        "sales_with_products.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Créer une vue temporaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "SELECT \r\n",
        "    ProductName, \r\n",
        "    SUM(TotalCost) AS total_revenue\r\n",
        "FROM \r\n",
        "    sales_view\r\n",
        "GROUP BY \r\n",
        "    ProductName"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Exécution de la requête SQL et stockage du résultat\r\n",
        "df_top_products = spark.sql(\"\"\"\r\n",
        "    SELECT \r\n",
        "        ProductName, \r\n",
        "        SUM(TotalCost) AS total_revenue\r\n",
        "    FROM \r\n",
        "        sales_view\r\n",
        "    GROUP BY \r\n",
        "        ProductName\r\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Trier et afficher les 10 produits les plus vendus\r\n",
        "df_top_10_products = df_top_products.orderBy(\"total_revenue\", ascending=False).limit(10)\r\n",
        "display(df_top_10_products.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Cas Pratique 5 - Spécification de schéma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Définir un schéma explicite pour la table Customer\r\n",
        "\r\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\r\n",
        "\r\n",
        "customer_schema = StructType([\r\n",
        "    StructField(\"CustomerKey\", IntegerType(), True),\r\n",
        "    StructField(\"FirstName\", StringType(), True),\r\n",
        "    StructField(\"LastName\", StringType(), True),\r\n",
        "    StructField(\"YearlyIncome\", DoubleType(), True),\r\n",
        "    StructField(\"TotalChildren\", IntegerType(), True)\r\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Lire un fichier CSV en appliquant un schéma custom\r\n",
        "\r\n",
        "df_dim_customer_custom = spark.read.csv(\r\n",
        "    path=dim_customer_path,\r\n",
        "    schema=customer_schema,\r\n",
        "    header=True,\r\n",
        "    sep=\",\",\r\n",
        "    inferSchema=False\r\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# Afficher le schéma et les 5 premières lignes\r\n",
        "\r\n",
        "df_dim_customer_custom.printSchema()\r\n",
        "df_dim_customer_custom.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Cas Pratique 6 - Utilisation des jointures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "outputs": [],
      "metadata": {},
      "source": [
        "# 1. Lister les clients avec leurs achats\r\n",
        "\r\n",
        "# Jointure entre clients et ventes\r\n",
        "clients_avec_achats = df_dim_customer.join(\r\n",
        "    df_factSales, \r\n",
        "    on=\"CustomerKey\", \r\n",
        "    how=\"inner\"\r\n",
        ")\r\n",
        "\r\n",
        "# Sélection des colonnes demandées\r\n",
        "clients_avec_achats.select(\r\n",
        "    \"FirstName\", \"LastName\", \"SalesOrderNumber\", \"SalesAmount\"\r\n",
        ").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# 2. Identifier les clients sans aucun achat\r\n",
        "\r\n",
        "clients_sans_achat = df_dim_customer.join(\r\n",
        "    df_factSales, \r\n",
        "    on=\"CustomerKey\", \r\n",
        "    how=\"left_anti\"\r\n",
        ")\r\n",
        "\r\n",
        "clients_sans_achat.select(\"FirstName\", \"LastName\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "outputs": [],
      "metadata": {},
      "source": [
        "# 3. Ajouter le nom du produit acheté (jointure avec Product)\r\n",
        "\r\n",
        "# Jointure à trois tables : Customer + OnlineSales + Product\r\n",
        "clients_achats_produits = df_dim_customer \\\r\n",
        "    .join(df_factSales, on=\"CustomerKey\", how=\"inner\") \\\r\n",
        "    .join(df_dim_product, on=\"ProductKey\", how=\"inner\")\r\n",
        "\r\n",
        "# Sélection enrichie avec le nom du produit\r\n",
        "clients_achats_produits.select(\r\n",
        "    \"FirstName\", \"LastName\", \"SalesOrderNumber\", \"SalesAmount\", \"ProductName\"\r\n",
        ").show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Cas Pratique 7 - Cas pratique complet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Sauvegarde de résultats en mode parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "clients_achats_produits.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "outputs": [],
      "metadata": {},
      "source": [
        "new_df = clients_achats_produits[['ProductKey', 'CustomerKey', '_c0', 'GeographyKey', 'FirstName', 'LastName', 'BirthDate', 'MaritalStatus',\r\n",
        " 'Gender', 'YearlyIncome', 'TotalChildren', 'NumberChildrenAtHome', 'Education']]\r\n",
        "\r\n",
        "new_df.printSchema()\r\n",
        "\r\n",
        "new_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### mode d'enregistrement des dataframes en format parquet : \r\n",
        "    - overwrite,\r\n",
        "    - append,\r\n",
        "    - ignore,\r\n",
        "    - error\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "outputs": [],
      "metadata": {},
      "source": [
        "output_dataset_path = \"abfss://dlsfssynwformwtwfrctrl@dlsaccsynwformwtwfrctrl.dfs.core.windows.net/synapse/workspaces/synw-formwtw-frctrl/warehouse/dataset/out_contoso/\"\r\n",
        "\r\n",
        "new_df.write.mode(\"overwrite\").parquet(output_dataset_path + \"CA_par_produits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Cas Pratique 8 - Cas pratique complet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### 1. Chargement des fichiers CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Chargement des fichiers CSV\r\n",
        "\r\n",
        "try:\r\n",
        "    dim_customer_df = spark.read.csv(dim_customer_path, header=True, inferSchema=True)\r\n",
        "    dim_product_df = spark.read.csv(dim_product_path, header=True, inferSchema=True)\r\n",
        "    dim_store_df = spark.read.csv(dim_store_path, header=True, inferSchema=True)\r\n",
        "    fact_sales_df = spark.read.csv(dim_factSales_path, header=True, inferSchema=True)\r\n",
        "except Exception as e:\r\n",
        "    print(f\"Error loading data: {e}\")\r\n",
        "    print(\"Please ensure the data_path is correct and the CSV files exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Chargement des fichiers CSV (suite)\r\n",
        "\r\n",
        "dim_date_path   = os.path.join(contoso_dataset_path, \"DimDate.csv\")\r\n",
        "dim_geo_path   = os.path.join(contoso_dataset_path, \"DimGeography.csv\")\r\n",
        "\r\n",
        "try:\r\n",
        "    dim_date_df = spark.read.csv(dim_date_path, header=True, inferSchema=True)\r\n",
        "    dim_geo_df = spark.read.csv(dim_geo_path, header=True, inferSchema=True)\r\n",
        "except Exception as e:\r\n",
        "    print(f\"Error loading data: {e}\")\r\n",
        "    print(\"Please ensure the data_path is correct and the CSV files exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### 2. Nettoyage et Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "outputs": [],
      "metadata": {},
      "source": [
        "fact_sales_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Supprimer les lignes avec des valeurs nulles critiques\r\n",
        "\r\n",
        "fact_sales_df = fact_sales_df.dropna(subset=[\"ProductKey\", \"DateKey\", \"SalesAmount\"])\r\n",
        "\r\n",
        "# Convertir les dates (dans DimDate) et montants (SalesAmount)\r\n",
        "\r\n",
        "from pyspark.sql.functions import to_date, col, concat_ws\r\n",
        "\r\n",
        "# Conversion de la date\r\n",
        "\"\"\"\r\n",
        "dim_date_df = dim_date_df.withColumn(\r\n",
        "    \"FullDate\", to_date(concat_ws(\"-\", col(\"CalendarYear\"), col(\"MonthNumber\"), col(\"DateKey\")), \"yyyy-M-d\")\r\n",
        ")\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "dim_date_df = dim_date_df.withColumn(\r\n",
        "    \"FullDate\", to_date(concat_ws(\"-\", col(\"DateKey\")), \"yyyy-M-d\")\r\n",
        ")\r\n",
        "\r\n",
        "# Ajout colonne YearMonth (ex : 2024-06)\r\n",
        "dim_date_df = dim_date_df.withColumn(\r\n",
        "    \"YearMonth\", concat_ws(\"-\", col(\"Year\"), col(\"Month\"))\r\n",
        ")\r\n",
        "\r\n",
        "# Conversion des montants en float si nécessaire\r\n",
        "fact_sales_df = fact_sales_df.withColumn(\"SalesAmount\", col(\"SalesAmount\").cast(\"double\"))\r\n",
        "\r\n",
        "display(dim_date_df.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### 3. Jointure — Création de la vue enrichie des ventes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "\r\n",
        "sales_enriched_df = fact_sales_df \\\r\n",
        "    .join(dim_date_df.select(\"DateKey\", \"FullDate\", \"YearMonth\"), on=\"DateKey\", how=\"inner\") \\\r\n",
        "    .join(dim_customer_df.select(\"CustomerKey\", \"GeographyKey\"), on=\"CustomerKey\", how=\"left\") \\\r\n",
        "    .join(dim_geo_df.select(\"GeographyKey\", \"RegionCountryName\"), on=\"GeographyKey\", how=\"left\") \\\r\n",
        "    .join(dim_product_df.select(\"ProductKey\", \"ProductName\", \"ProductCategoryName\"), on=\"ProductKey\", how=\"left\") \\\r\n",
        "    .join(dim_store_df.select(\"StoreKey\", \"StoreName\", \"StoreType\"), on=\"StoreKey\", how=\"left\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### 4.  KPI à Calculer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# KPI 1 – Chiffre d'affaires mensuel par produit et région\r\n",
        "\r\n",
        "monthly_revenue = sales_enriched_df.groupBy(\"YearMonth\", \"ProductName\", \"RegionCountryName\") \\\r\n",
        "    .agg({\"SalesAmount\": \"sum\"}) \\\r\n",
        "    .withColumnRenamed(\"sum(SalesAmount)\", \"MonthlyRevenue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# KPI 2 – Top 10 produits les plus vendus (volume et valeur)\r\n",
        "\r\n",
        "from pyspark.sql.functions import sum as _sum\r\n",
        "\r\n",
        "top_products = sales_enriched_df.groupBy(\"ProductName\") \\\r\n",
        "    .agg(\r\n",
        "        _sum(\"SalesAmount\").alias(\"TotalSales\"),\r\n",
        "        _sum(\"SalesQuantity\").alias(\"TotalUnits\")\r\n",
        "    ) \\\r\n",
        "    .orderBy(col(\"TotalSales\").desc())\r\n",
        "\r\n",
        "top_products.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# KPI 3 – Taux de croissance mensuel des ventes\r\n",
        "\r\n",
        "from pyspark.sql.window import Window\r\n",
        "from pyspark.sql.functions import lag, round\r\n",
        "\r\n",
        "monthly_sales = sales_enriched_df.groupBy(\"YearMonth\").agg(_sum(\"SalesAmount\").alias(\"TotalSales\"))\r\n",
        "\r\n",
        "w = Window.orderBy(\"YearMonth\")\r\n",
        "\r\n",
        "monthly_growth = monthly_sales.withColumn(\r\n",
        "    \"PreviousMonthSales\", lag(\"TotalSales\").over(w)\r\n",
        ").withColumn(\r\n",
        "    \"GrowthRate\", round((col(\"TotalSales\") - col(\"PreviousMonthSales\")) / col(\"PreviousMonthSales\") * 100, 2)\r\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# KPI 4 – Répartition des ventes par canal (magasin physique vs en ligne)\r\n",
        "\r\n",
        "channel_split = sales_enriched_df.groupBy(\"StoreType\") \\\r\n",
        "    .agg(_sum(\"SalesAmount\").alias(\"TotalSales\")) \\\r\n",
        "    .orderBy(\"TotalSales\", ascending=False)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# KPI 5 – Analyse RFM (Récence, Fréquence, Montant)\r\n",
        "\r\n",
        "from pyspark.sql.functions import max, datediff, countDistinct\r\n",
        "\r\n",
        "# Dernière date d'achat\r\n",
        "latest_date = sales_enriched_df.agg({\"FullDate\": \"max\"}).collect()[0][0]\r\n",
        "\r\n",
        "rfm = sales_enriched_df.groupBy(\"CustomerKey\") \\\r\n",
        "    .agg(\r\n",
        "        datediff(lit(latest_date), max(\"FullDate\")).alias(\"Recency\"),\r\n",
        "        countDistinct(\"SalesOrderNumber\").alias(\"Frequency\"),\r\n",
        "        _sum(\"SalesAmount\").alias(\"Monetary\")\r\n",
        "    )\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.types import IntegerType, UserDefinedType\r\n",
        "import random\r\n",
        "random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Have fun with PySpark !"
      ]
    }
  ],
  "metadata": {
    "description": "DataFrame Operations - WTW PySpark Session",
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}